{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning pour la classification de tweets\n",
    "*Saber Zerhoudi*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Introduction\n",
    "L’objectif est l’analyse et la détection automatique de sujets sur un corpus ou un flux de tweets. Les méthodes employées sont celles utilisées couramment en Machine Learning supervisé et non supervisé. L’approche choisie utilise une première phase de sélection basée sur le traitement supervisé des Hashtags puis une phase de traitement automatisé de l’ensemble des données textuelles. Pour finir, on test des méthodes de sélection des tweets les plus importants pour chacun des topics pour définir celle à utiliser.\n",
    "\n",
    "\n",
    "## Bibliothèques Python utilisées\n",
    "Les données ont été traitées en utilisant  les bibliothèques suivantes *scipy* et *numpy* et le processus d'apprentissage/validation a été construit avec *scikit-learn* et *libact*. Les diagrammes ont été créées en utilisant *matplotlib* et *seaborn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from strategies.strategies import USampling, CMBSampling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from libact.base.dataset import Dataset\n",
    "from libact.models import LogisticRegression, SVM\n",
    "from libact.query_strategies import UncertaintySampling, RandomSampling, QueryByCommittee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source de données\n",
    "\n",
    "Les données comprenaient trois fichiers CSV:\n",
    "`corpus_2015_id-time-text.csv` (30911 tweets) et `oriane_pos_id-time-text.csv` qui contient que ceux qui sont positives et `oriane_neg_id-time-text.csv` qui contient ceux négative.\n",
    "\n",
    "Le format des fichiers CSV était le suivant :\n",
    "\n",
    "| Id | Timestamp  | Tweet |\n",
    "|------|------|------|\n",
    "|   673860306414759936  | 1449495791425 | On reste calme et on suit le live-tweet de la #FakeDesLumièresn#leresistant https://t.co/l2AbZysqjS |\n",
    "\n",
    "Les données comprenaient aussi trois fichiers TXT qui correspondent respectivement au contenu des fichiers CSV après avoir vectoriser les tweets à l'aide de W2V :\n",
    "`vectors_2015.txt` (30911 tweets) et `vectors_2015_pos.txt` qui contient que ceux qui sont positives et `vectors_2015_neg.txt` qui contient ceux négative.\n",
    "\n",
    "Le format des fichiers TXT était le suivant :\n",
    "\n",
    "| W2V (dim 100) | Id | Timestamp  | Tweet |\n",
    "|------|------|------|------|\n",
    "|   [ -4.57972735e-02  ...  -5.48090134e-03]  |   673860306414759936  | 1449495791425 | On reste calme et on suit le live-tweet de la #FakeDesLumièresn#leresistant https://t.co/l2AbZysqjS |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement de données\n",
    "## Fonctions utilisés en main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ouvrir un fichier en format txt\n",
    "def openfile_txt(filepath):\n",
    "    with open(filepath, 'r', encoding='utf16') as f:\n",
    "        file = f.read().split('\"\\n[')\n",
    "    return file\n",
    "\n",
    "# Simulation du w2v : étant donné un tweet_id on retourne le vecteur associé\n",
    "def simulate_w4v(tweet_id):\n",
    "    element_id = ids_list.index(tweet_id)\n",
    "    vectorized = vectors_list[element_id]\n",
    "    return vectorized\n",
    "\n",
    "# Parcours le fichier des vecteurs en format txt et stock les vecteurs et les ids sur deux listes \n",
    "def get_vectors_list(filepath):\n",
    "    vectors_list_x, ids_list_x = [], []\n",
    "    with open(filepath, 'r', encoding='utf16') as f:\n",
    "        file = f.read().split('\"\\n[')\n",
    "        for line in file:\n",
    "            parts = line.replace('\\n', '').replace('    ', ' ').replace('  ', ' ').replace('  ', ' ').split(\";\")\n",
    "            vectors_list_x.append(parts[0])\n",
    "            ids_list_x.append(parts[1].replace(' ', ''))\n",
    "    return vectors_list_x, ids_list_x\n",
    "\n",
    "# À partir d'un tweet_id on vérifie s'il est positif ou négatif\n",
    "def define_label(tweet_id):\n",
    "    with open(pos_filepath, 'r', encoding='utf16') as f:\n",
    "        next(f)\n",
    "        for line in f.readlines():\n",
    "            parts = line.split(\";\")\n",
    "            tweets = parts[0].replace('\"', '')\n",
    "            if tweet_id in tweets:\n",
    "                label = 1\n",
    "                break\n",
    "            else:\n",
    "                label = 0\n",
    "    return label\n",
    "\n",
    "# À partir d'un tweet_id on retourne le Tweet associé\n",
    "def define_tweet_by_id(line_id):\n",
    "    with open(csv_filepath, 'r', encoding='utf16') as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            if i == line_id:\n",
    "                parts = line.split(\";\")\n",
    "                tweet = parts[2]\n",
    "            elif i > line_id:\n",
    "                break\n",
    "    return tweet\n",
    "\n",
    "# On randomise les listes des datas et labels X et y tout on gardant le même ordre des couples de (X, y)\n",
    "def randomize(X, y):\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    X2 = X[permutation]\n",
    "    y2 = y[permutation]\n",
    "    return X2, y2\n",
    "\n",
    "# On construit les targets (labels) et datas (vecteurs) à partir du fichier corpus CSV\n",
    "def build_dataset(file):\n",
    "    target, data = [], []\n",
    "    for line in file:\n",
    "        z = np.array(define_label(line[1].replace(' ', '')))\n",
    "        target.append(z)\n",
    "        x = np.fromstring(line[0].replace(']', '').replace('[', '').replace('  ', ' '), sep=' ')\n",
    "        data.append(x)\n",
    "    target = np.asarray(target)\n",
    "    data = np.asarray(data)\n",
    "    return target, data\n",
    "\n",
    "# En se basant sur un Dataset déséquilibré, on crée un Dataset équilibré qui contient 1000 Tweet pos et 1000 Tweet neg au hasard\n",
    "def balance_dataset():\n",
    "    file_pos_ids, file_neg_ids = [], []\n",
    "    file_pos = openfile_txt(pos_filepath_txt)\n",
    "    for line in file_pos:\n",
    "        parts = line.replace('\\n', '').replace('    ', ' ').replace('  ', ' ').replace('  ', ' ').split(\";\")\n",
    "        file_pos_ids.append(parts)\n",
    "    pos_part = random.sample(file_pos_ids, 1000)\n",
    "\n",
    "    file_neg = openfile_txt(neg_filepath_txt)\n",
    "    for line in file_neg:\n",
    "        parts = line.replace('\\n', '').replace('    ', ' ').replace('  ', ' ').replace('  ', ' ').split(\";\")\n",
    "        file_neg_ids.append(parts)\n",
    "    neg_part = random.sample(file_neg_ids, 1000)\n",
    "\n",
    "    balanced_txt_file = pos_part+neg_part\n",
    "    random.shuffle(balanced_txt_file)\n",
    "    return balanced_txt_file\n",
    "\n",
    "# Simuler l'interaction Homme-Machine pour définir le label des Tweets : Pour chaque Tweet choisi on cherche son label (pos/neg)\n",
    "def simulate_human_decision(line_id):\n",
    "    with open(csv_filepath, 'r', encoding='utf16') as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            if i == line_id:\n",
    "                parts = line.split(\";\")\n",
    "                tweet_id = parts[0]\n",
    "                label = define_label(tweet_id)\n",
    "            elif i > line_id:\n",
    "                break\n",
    "    return label\n",
    "\n",
    "# On split le Dataset en train et test en partant de 50 tweets labelés\n",
    "def split_train_test(file):\n",
    "    target = build_dataset(file)\n",
    "    n_labeled = 50\n",
    "\n",
    "    X = target[1]\n",
    "    y = target[0]\n",
    "    print(np.shape(X))\n",
    "    print(np.shape(y))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    while (np.sum(y_train[:n_labeled]) < 25):\n",
    "        X_rand, y_rand = randomize(X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_rand, y_rand, test_size=0.2, stratify=y_rand)\n",
    "\n",
    "    print(np.concatenate([y_train[:n_labeled], [None] * (len(y_train) - n_labeled)]))\n",
    "\n",
    "    trn_ds = Dataset(X_train, np.concatenate([y_train[:n_labeled], [None] * (len(y_train) - n_labeled)]))\n",
    "    tst_ds = Dataset(X_test, y_test)\n",
    "\n",
    "    return trn_ds, tst_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction main()\n",
    "\n",
    "En partant d'un jeu de donnée qui contient 2000 Tweets (dont 1000 sont pos et 1000 sont neg) on définit 50 Tweets labelés et on simule les 100 prochains. Le choix des Tweets est définit à chaque fois en utilisant un algorithme de classification différent ainsi que des méthodes de décisions différentes.\n",
    "\n",
    "Les méthodes de classification testé sont : **RandomForest**, SVM(linear), LogisticRegression.\n",
    "Les algorithmes d'active learning testé sont : UncertaintySampling, CMB Sampling(Combination of active learning algorithms (distance-based (DIST), diversity-based (DIV))), Random Sampling, **QueryByCommittee**, QUIRE.\n",
    "Et les fonctions de décisions pour le choix des queries utilisé sont : **Vote Entropy, Kullback-Leibler Divergence** , distance-based (DIST), diversity-based (DIV), Max Margin, Least Confident\n",
    "\n",
    "En sortie, on affiche la courbe de l'évolution de l'Accuracy par rapport au nombre de queries traitées et un fichier TXT qui contient les différentes queries traitées à chaque étape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    global pos_filepath, neg_filepath_txt, pos_filepath_txt, dataset_filepath, csv_filepath, vectors_list, ids_list\n",
    "    # Les chemins de fichier à changer\n",
    "    dataset_filepath = \"/Users/dndesign/Desktop/active_learning/vecteurs_et_infos/vectors_2015.txt\"\n",
    "    csv_filepath = \"/Users/dndesign/Desktop/active_learning/donnees/corpus_2015_id-time-text.csv\"\n",
    "    pos_filepath = \"/Users/dndesign/Desktop/active_learning/donnees/oriane_pos_id-time-text.csv\"\n",
    "    pos_filepath_txt = \"/Users/dndesign/Desktop/active_learning/vecteurs_et_infos/vectors_2015_pos.txt\"\n",
    "    neg_filepath_txt = \"/Users/dndesign/Desktop/active_learning/vecteurs_et_infos/vectors_2015_neg.txt\"\n",
    "    vectors_list, ids_list = get_vectors_list(dataset_filepath)\n",
    "\n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    text_file = codecs.open(\"task_\" + str(timestr) + \".txt\", \"w\", \"utf-8\")\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    text_file.write(\"Loading data...\\n\")\n",
    "    # Open this file\n",
    "    t0 = time.time()\n",
    "    file = openfile_txt(dataset_filepath)\n",
    "    num_lines = sum(1 for line in file)\n",
    "    print(\"Treating \" + str(num_lines) + \" entries...\")\n",
    "    text_file.write(\"Treating : %s entries...\\n\" % str(num_lines))\n",
    "\n",
    "    # Number of queries to ask human to label\n",
    "    quota = 100\n",
    "    E_out1, E_out2, E_out3, E_out4, E_out6, E_out7 = [], [], [], [], [], []\n",
    "    balanced_file = balance_dataset()\n",
    "    trn_ds, tst_ds = split_train_test(balanced_file)\n",
    "\n",
    "    # model = SVM(kernel='linear')\n",
    "    # model = LogisticRegression()\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    ''' UncertaintySampling (Least Confident)\n",
    "\n",
    "        UncertaintySampling : it queries the instances about which \n",
    "        it is least certain how to label\n",
    "\n",
    "        Least Confident : it queries the instance whose posterior \n",
    "        probability of being positive is nearest 0.5\n",
    "    '''\n",
    "    qs = UncertaintySampling(trn_ds, method='lc', model=LogisticRegression(C=.01))\n",
    "    # model.train(trn_ds)\n",
    "    model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "    predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "    score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "    E_out1 = np.append(E_out1, 1 - score)\n",
    "    # E_out1 = np.append(E_out1, 1 - model.score(tst_ds))\n",
    "\n",
    "    ''' UncertaintySampling (Max Margin) \n",
    "\n",
    "    '''\n",
    "    trn_ds2 = copy.deepcopy(trn_ds)\n",
    "    qs2 = USampling(trn_ds2, method='mm', model=SVM(kernel='linear'))\n",
    "    # model.train(trn_ds2)\n",
    "    model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "    predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "    score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "    E_out2 = np.append(E_out2, 1 - score)\n",
    "    # E_out2 = np.append(E_out2, 1 - model.score(tst_ds))\n",
    "\n",
    "    ''' CMB Sampling   \n",
    "        Combination of active learning algorithms (distance-based (DIST), diversity-based (DIV)) \n",
    "    '''\n",
    "    trn_ds3 = copy.deepcopy(trn_ds)\n",
    "    qs3 = CMBSampling(trn_ds3, model=SVM(kernel='linear'))\n",
    "    # model.train(trn_ds3)\n",
    "    model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "    predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "    score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "    E_out3 = np.append(E_out3, 1 - score)\n",
    "    # E_out3 = np.append(E_out3, 1 - model.score(tst_ds))\n",
    "\n",
    "    ''' Random Sampling   \n",
    "        Random : it chooses randomly a query\n",
    "    '''\n",
    "    trn_ds4 = copy.deepcopy(trn_ds)\n",
    "    qs4 = RandomSampling(trn_ds4, random_state=1126)\n",
    "    # model.train(trn_ds4)\n",
    "    model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "    predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "    score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "    E_out4 = np.append(E_out4, 1 - score)\n",
    "    # E_out4 = np.append(E_out4, 1 - model.score(tst_ds))\n",
    "\n",
    "    ''' QueryByCommittee (Vote Entropy)\n",
    "\n",
    "        QueryByCommittee : it keeps a committee of classifiers and queries \n",
    "        the instance that the committee members disagree, it  also examines \n",
    "        unlabeled examples and selects only those that are most informative \n",
    "        for labeling\n",
    "\n",
    "        Vote Entropy : a way of measuring disagreement \n",
    "\n",
    "        Disadvantage : it does not consider the committee members’ class \n",
    "        distributions. It also misses some informative unlabeled examples \n",
    "        to label \n",
    "    '''\n",
    "    trn_ds6 = copy.deepcopy(trn_ds)\n",
    "    qs6 = QueryByCommittee(trn_ds6, disagreement='vote',\n",
    "                           models=[LogisticRegression(C=1.0),\n",
    "                                   LogisticRegression(C=0.01),\n",
    "                                   LogisticRegression(C=100)],\n",
    "                           random_state=1126)\n",
    "    # model.train(trn_ds6)\n",
    "    model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "    predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "    score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "    E_out6 = np.append(E_out6, 1 - score)\n",
    "    # E_out6 = np.append(E_out6, 1 - model.score(tst_ds))\n",
    "\n",
    "    ''' QueryByCommittee (Kullback-Leibler Divergence)\n",
    "\n",
    "            QueryByCommittee : it examines unlabeled examples and selects only \n",
    "            those that are most informative for labeling\n",
    "\n",
    "            Disadvantage :  it misses some examples on which committee members \n",
    "            disagree\n",
    "    '''\n",
    "    trn_ds7 = copy.deepcopy(trn_ds)\n",
    "    qs7 = QueryByCommittee(trn_ds7, disagreement='kl_divergence',\n",
    "                           models=[LogisticRegression(C=1.0),\n",
    "                                   LogisticRegression(C=0.01),\n",
    "                                   LogisticRegression(C=100)],\n",
    "                           random_state=1126)\n",
    "    # model.train(trn_ds7)\n",
    "    model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "    predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "    score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "    E_out7 = np.append(E_out7, 1 - score)\n",
    "    # E_out7 = np.append(E_out7, 1 - model.score(tst_ds))\n",
    "\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    query_num = np.arange(0, 1)\n",
    "    p1, = ax.plot(query_num, E_out1, 'red')\n",
    "    p2, = ax.plot(query_num, E_out2, 'blue')\n",
    "    p3, = ax.plot(query_num, E_out3, 'green')\n",
    "    p4, = ax.plot(query_num, E_out4, 'orange')\n",
    "    p6, = ax.plot(query_num, E_out6, 'black')\n",
    "    p7, = ax.plot(query_num, E_out7, 'purple')\n",
    "    plt.legend(\n",
    "        ('Least Confident', 'Max Margin', 'Distance Diversity CMB', 'Random Sampling', 'Vote Entropy', 'KL Divergence'),\n",
    "        loc=1)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Number of Queries')\n",
    "    plt.title('Active Learning - Query choice strategies')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show(block=False)\n",
    "\n",
    "    for i in range(quota):\n",
    "        print(\"\\n#################################################\")\n",
    "        print(\"Query number \" + str(i) + \" : \")\n",
    "        print(\"#################################################\\n\")\n",
    "        text_file.write(\"\\n#################################################\\n\")\n",
    "        text_file.write(\"Query number %s : \" % str(i))\n",
    "        text_file.write(\"\\n#################################################\\n\")\n",
    "\n",
    "        ask_id = qs.make_query()\n",
    "        print(\"\\033[4mUsing Uncertainty Sampling (Least confident) :\\033[0m\")\n",
    "        print(\"Tweet :\" + define_tweet_by_id(ask_id), end='', flush=True)\n",
    "        print(\"Simulating human response : \" + str(simulate_human_decision(ask_id)) + \" \\n\")\n",
    "        text_file.write(\"Using Uncertainty Sampling (Least confident) :\\n\")\n",
    "        text_file.write(\"Tweet : %s \\n\" % str(define_tweet_by_id(ask_id)))\n",
    "        text_file.write(\"Simulating human response : %s \\n\\n\" % str(simulate_human_decision(ask_id)))\n",
    "        trn_ds.update(ask_id, simulate_human_decision(ask_id))\n",
    "        # model.train(trn_ds)\n",
    "        model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "        predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "        score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "        E_out1 = np.append(E_out1, 1 - score)\n",
    "        # E_out1 = np.append(E_out1, 1 - model.score(tst_ds))\n",
    "\n",
    "        ask_id = qs2.make_query()\n",
    "        print(\"\\033[4mUsing Uncertainty Sampling (Max Margin) :\\033[0m\")\n",
    "        print(\"Tweet :\" + define_tweet_by_id(ask_id), end='', flush=True)\n",
    "        print(\"Simulating human response : \" + str(simulate_human_decision(ask_id)) + \" \\n\")\n",
    "        text_file.write(\"Using Uncertainty Sampling (Smallest Margin) :\\n\")\n",
    "        text_file.write(\"Tweet : %s \\n\" % str(define_tweet_by_id(ask_id)))\n",
    "        text_file.write(\"Simulating human response : %s \\n\\n\" % str(simulate_human_decision(ask_id)))\n",
    "        trn_ds2.update(ask_id, simulate_human_decision(ask_id))\n",
    "        # model.train(trn_ds2)\n",
    "        model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "        predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "        score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "        E_out2 = np.append(E_out2, 1 - score)\n",
    "        # E_out2 = np.append(E_out2, 1 - model.score(tst_ds))\n",
    "\n",
    "        ask_id = qs3.make_query()\n",
    "        print(\"\\033[4mUsing CMB Distance-Diversity Sampling :\\033[0m\")\n",
    "        print(\"Tweet :\" + define_tweet_by_id(ask_id), end='', flush=True)\n",
    "        print(\"Simulating human response : \" + str(simulate_human_decision(ask_id)) + \" \\n\")\n",
    "        text_file.write(\"Using Uncertainty Sampling (Entropy) :\\n\")\n",
    "        text_file.write(\"Tweet : %s \\n\" % str(define_tweet_by_id(ask_id)))\n",
    "        text_file.write(\"Simulating human response : %s \\n\\n\" % str(simulate_human_decision(ask_id)))\n",
    "        trn_ds3.update(ask_id, simulate_human_decision(ask_id))\n",
    "        # model.train(trn_ds3)\n",
    "        model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "        predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "        score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "        E_out3 = np.append(E_out3, 1 - score)\n",
    "        # E_out3 = np.append(E_out3, 1 - model.score(tst_ds))\n",
    "\n",
    "        ask_id = qs4.make_query()\n",
    "        print(\"\\033[4mUsing Random Sampling :\\033[0m\")\n",
    "        print(\"Tweet :\" + define_tweet_by_id(ask_id), end='', flush=True)\n",
    "        print(\"Simulating human response : \" + str(simulate_human_decision(ask_id)) + \" \\n\")\n",
    "        text_file.write(\"Using Random Sampling :\\n\")\n",
    "        text_file.write(\"Tweet : %s \\n\" % str(define_tweet_by_id(ask_id)))\n",
    "        text_file.write(\"Simulating human response : %s \\n\\n\" % str(simulate_human_decision(ask_id)))\n",
    "        trn_ds4.update(ask_id, simulate_human_decision(ask_id))\n",
    "        # model.train(trn_ds4)\n",
    "        model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "        predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "        score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "        E_out4 = np.append(E_out4, 1 - score)\n",
    "        # E_out4 = np.append(E_out4, 1 - model.score(tst_ds))\n",
    "\n",
    "        ask_id = qs6.make_query()\n",
    "        print(\"\\033[4mUsing QueryByCommittee (Vote Entropy) :\\033[0m\")\n",
    "        print(\"Tweet :\" + define_tweet_by_id(ask_id), end='', flush=True)\n",
    "        print(\"Simulating human response : \" + str(simulate_human_decision(ask_id)) + \" \\n\")\n",
    "        text_file.write(\"Using QueryByCommittee (Vote Entropy) :\\n\")\n",
    "        text_file.write(\"Tweet : %s \\n\" % str(define_tweet_by_id(ask_id)))\n",
    "        text_file.write(\"Simulating human response : %s \\n\\n\" % str(simulate_human_decision(ask_id)))\n",
    "        trn_ds6.update(ask_id, simulate_human_decision(ask_id))\n",
    "        # model.train(trn_ds6)\n",
    "        model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "        predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "        score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "        E_out6 = np.append(E_out6, 1 - score)\n",
    "        # E_out6 = np.append(E_out6, 1 - model.score(tst_ds))\n",
    "\n",
    "        ask_id = qs7.make_query()\n",
    "        print(\"\\033[4mUsing QueryByCommittee (KL Divergence) :\\033[0m\")\n",
    "        print(\"Tweet :\" + define_tweet_by_id(ask_id), end='', flush=True)\n",
    "        print(\"Simulating human response : \" + str(simulate_human_decision(ask_id)) + \" \\n\")\n",
    "        text_file.write(\"Using QueryByCommittee (KL Divergence) :\\n\")\n",
    "        text_file.write(\"Tweet : %s \\n\" % str(define_tweet_by_id(ask_id)))\n",
    "        text_file.write(\"Simulating human response : %s \\n\\n\" % str(simulate_human_decision(ask_id)))\n",
    "        trn_ds7.update(ask_id, simulate_human_decision(ask_id))\n",
    "        # model.train(trn_ds7)\n",
    "        model.fit(trn_ds.format_sklearn()[0], trn_ds.format_sklearn()[1])\n",
    "        predicted = model.predict(tst_ds.format_sklearn()[0])\n",
    "        score = accuracy_score(tst_ds.format_sklearn()[1], predicted)\n",
    "        E_out7 = np.append(E_out7, 1 - score)\n",
    "        # E_out7 = np.append(E_out7, 1 - model.score(tst_ds))\n",
    "\n",
    "        ax.set_xlim((0, i + 1))\n",
    "        ax.set_ylim((0, max(max(E_out1), max(E_out2), max(E_out3), max(E_out4), max(E_out6), max(E_out7)) + 0.2))\n",
    "        query_num = np.arange(0, i + 2)\n",
    "        p1.set_xdata(query_num)\n",
    "        p1.set_ydata(E_out1)\n",
    "        p2.set_xdata(query_num)\n",
    "        p2.set_ydata(E_out2)\n",
    "        p3.set_xdata(query_num)\n",
    "        p3.set_ydata(E_out3)\n",
    "        p4.set_xdata(query_num)\n",
    "        p4.set_ydata(E_out4)\n",
    "        p6.set_xdata(query_num)\n",
    "        p6.set_ydata(E_out6)\n",
    "        p7.set_xdata(query_num)\n",
    "        p7.set_ydata(E_out7)\n",
    "\n",
    "        plt.draw()\n",
    "\n",
    "    t2 = time.time()\n",
    "    time_total = t2 - t0\n",
    "    print(\"\\n\\n\\n#################################################\\n\")\n",
    "    print(\"Execution time : %fs \\n\\n\" % time_total)\n",
    "    text_file.write(\"\\n\\n\\n#################################################\\n\")\n",
    "    text_file.write(\"Execution time : %fs \\n\" % time_total)\n",
    "    text_file.close()\n",
    "    input(\"Press any key to save the plot...\")\n",
    "    plt.savefig('task_' + str(timestr) + '.png')\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats et Remarques\n",
    "\n",
    "Partant d'un Dataset (30911 Tweets) très déséquilibré avec 10 queries déjà labelées et en simulant 10 queries pour les labeler, on se retrouve avec le résultat suivant :\n",
    "<img src=\"task_20180102_024943.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et quand on essaie d'équilibrer le jeu de donnée en partant de 1000 Tweets positifs et 1000 négatifs pour un total de 2000 queries random, et que cette fois-ci on part de 50 queries déjà labelées et on simule 100 queries à labeler, on arrive au résultat suivant :\n",
    "### Random Forest\n",
    "<img src=\"task_20180117_143134.png\">\n",
    "### SVM(linear)\n",
    "<img src=\"task_20180113_220114.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
